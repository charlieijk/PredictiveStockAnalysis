{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Advanced Feature Engineering Module\n",
        "Handles feature creation, selection, and preprocessing for ML models\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "import logging\n",
        "from scipy import stats\n",
        "try:\n",
        "    import talib\n",
        "    TALIB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TALIB_AVAILABLE = False\n",
        "    logging.warning(\"TA-Lib not available. Using basic pattern detection.\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for stock price prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, scaling_method: str = 'robust'):\n",
        "        \"\"\"\n",
        "        Initialize the feature engineer\n",
        "        \n",
        "        Args:\n",
        "            scaling_method: Method for scaling features ('standard', 'minmax', 'robust')\n",
        "        \"\"\"\n",
        "        self.scaling_method = scaling_method\n",
        "        self.scaler = self._get_scaler(scaling_method)\n",
        "        self.feature_names = None\n",
        "        self.selected_features = None\n",
        "        self.pca = None\n",
        "        \n",
        "    def _get_scaler(self, method: str):\n",
        "        \"\"\"Get the appropriate scaler based on method\"\"\"\n",
        "        scalers = {\n",
        "            'standard': StandardScaler(),\n",
        "            'minmax': MinMaxScaler(),\n",
        "            'robust': RobustScaler()\n",
        "        }\n",
        "        return scalers.get(method, RobustScaler())\n",
        "    \n",
        "    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create advanced technical features\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame with OHLCV data\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with additional advanced features\n",
        "        \"\"\"\n",
        "        logger.info(\"Creating advanced technical features...\")\n",
        "        \n",
        "        # Ensure we have necessary columns\n",
        "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            raise ValueError(f\"DataFrame must contain columns: {required_cols}\")\n",
        "        \n",
        "        # Price-based features\n",
        "        df = self._create_price_features(df)\n",
        "        \n",
        "        # Volume-based features\n",
        "        df = self._create_volume_features(df)\n",
        "        \n",
        "        # Volatility features\n",
        "        df = self._create_volatility_features(df)\n",
        "        \n",
        "        # Pattern recognition features\n",
        "        df = self._create_pattern_features(df)\n",
        "        \n",
        "        # Market microstructure features\n",
        "        df = self._create_microstructure_features(df)\n",
        "        \n",
        "        # Momentum indicators\n",
        "        df = self._create_momentum_features(df)\n",
        "        \n",
        "        # Statistical features\n",
        "        df = self._create_statistical_features(df)\n",
        "        \n",
        "        logger.info(f\"Created {len(df.columns)} total features\")\n",
        "        return df\n",
        "    \n",
        "    def _create_price_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create price-based features\"\"\"\n",
        "        # Price ratios\n",
        "        df['HL_Ratio'] = df['High'] / df['Low']\n",
        "        df['CO_Ratio'] = df['Close'] / df['Open']\n",
        "        df['Volume_Ratio'] = df['Volume'] / df['Volume'].rolling(20).mean()\n",
        "        \n",
        "        # Gap features\n",
        "        df['Gap'] = df['Open'] - df['Close'].shift(1)\n",
        "        df['Gap_Percentage'] = (df['Gap'] / df['Close'].shift(1)) * 100\n",
        "        \n",
        "        # Price position within day's range\n",
        "        df['Price_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])\n",
        "        \n",
        "        # Distance from various MAs\n",
        "        for period in [10, 20, 50]:\n",
        "            ma = df['Close'].rolling(period).mean()\n",
        "            df[f'Distance_MA{period}'] = (df['Close'] - ma) / ma * 100\n",
        "        \n",
        "        # Price channels\n",
        "        df['Highest_20'] = df['High'].rolling(20).max()\n",
        "        df['Lowest_20'] = df['Low'].rolling(20).min()\n",
        "        df['Price_Channel_Position'] = (df['Close'] - df['Lowest_20']) / (df['Highest_20'] - df['Lowest_20'])\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create volume-based features\"\"\"\n",
        "        # Volume-weighted average price (VWAP)\n",
        "        df['VWAP'] = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).cumsum() / df['Volume'].cumsum()\n",
        "        df['Price_VWAP_Ratio'] = df['Close'] / df['VWAP']\n",
        "        \n",
        "        # Money Flow Index (MFI)\n",
        "        typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "        money_flow = typical_price * df['Volume']\n",
        "        \n",
        "        positive_flow = pd.Series(0, index=df.index)\n",
        "        negative_flow = pd.Series(0, index=df.index)\n",
        "        \n",
        "        for i in range(1, len(df)):\n",
        "            if typical_price.iloc[i] > typical_price.iloc[i-1]:\n",
        "                positive_flow.iloc[i] = money_flow.iloc[i]\n",
        "            else:\n",
        "                negative_flow.iloc[i] = money_flow.iloc[i]\n",
        "        \n",
        "        positive_flow_sum = positive_flow.rolling(14).sum()\n",
        "        negative_flow_sum = negative_flow.rolling(14).sum()\n",
        "        \n",
        "        mfi_ratio = positive_flow_sum / negative_flow_sum\n",
        "        df['MFI'] = 100 - (100 / (1 + mfi_ratio))\n",
        "        \n",
        "        # Accumulation/Distribution Line\n",
        "        clv = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])\n",
        "        df['AD_Line'] = (clv * df['Volume']).cumsum()\n",
        "        \n",
        "        # Chaikin Money Flow\n",
        "        df['CMF'] = (clv * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create volatility-based features\"\"\"\n",
        "        # Average True Range (ATR)\n",
        "        high_low = df['High'] - df['Low']\n",
        "        high_close = abs(df['High'] - df['Close'].shift())\n",
        "        low_close = abs(df['Low'] - df['Close'].shift())\n",
        "        \n",
        "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "        df['ATR'] = true_range.rolling(14).mean()\n",
        "        df['ATR_Percentage'] = (df['ATR'] / df['Close']) * 100\n",
        "        \n",
        "        # Normalized ATR\n",
        "        df['NATR'] = (df['ATR'] / df['Close']) * 100\n",
        "        \n",
        "        # Keltner Channels\n",
        "        ema20 = df['Close'].ewm(span=20).mean()\n",
        "        df['KC_Upper'] = ema20 + (df['ATR'] * 2)\n",
        "        df['KC_Lower'] = ema20 - (df['ATR'] * 2)\n",
        "        df['KC_Position'] = (df['Close'] - df['KC_Lower']) / (df['KC_Upper'] - df['KC_Lower'])\n",
        "        \n",
        "        # Historical Volatility (different periods)\n",
        "        for period in [5, 10, 20, 60]:\n",
        "            returns = df['Close'].pct_change()\n",
        "            df[f'HV_{period}'] = returns.rolling(period).std() * np.sqrt(252)\n",
        "        \n",
        "        # Parkinson Volatility\n",
        "        df['Parkinson_Vol'] = np.sqrt(\n",
        "            (1/(4*np.log(2))) * ((np.log(df['High']/df['Low']))**2).rolling(20).mean()\n",
        "        ) * np.sqrt(252)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_pattern_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create candlestick pattern features\"\"\"\n",
        "        # Using TA-Lib for pattern recognition if available\n",
        "        use_simple = True\n",
        "        if TALIB_AVAILABLE:\n",
        "            try:\n",
        "                # Bullish patterns\n",
        "                df['HAMMER'] = talib.CDLHAMMER(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "                df['MORNING_STAR'] = talib.CDLMORNINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "                df['BULLISH_ENGULFING'] = talib.CDLENGULFING(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "\n",
        "                # Bearish patterns\n",
        "                df['SHOOTING_STAR'] = talib.CDLSHOOTINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "                df['EVENING_STAR'] = talib.CDLEVENINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "                df['BEARISH_ENGULFING'] = talib.CDLENGULFING(df['Open'], df['High'], df['Low'], df['Close']) * -1\n",
        "\n",
        "                # Neutral/Reversal patterns\n",
        "                df['DOJI'] = talib.CDLDOJI(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "                df['SPINNING_TOP'] = talib.CDLSPINNINGTOP(df['Open'], df['High'], df['Low'], df['Close'])\n",
        "\n",
        "                use_simple = False\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"TA-Lib error: {str(e)}, using simple pattern detection\")\n",
        "\n",
        "        if use_simple:\n",
        "            # Simple pattern detection without TA-Lib\n",
        "            df['Body_Size'] = abs(df['Close'] - df['Open'])\n",
        "            df['Upper_Shadow'] = df['High'] - df[['Close', 'Open']].max(axis=1)\n",
        "            df['Lower_Shadow'] = df[['Close', 'Open']].min(axis=1) - df['Low']\n",
        "            df['Body_to_Shadow_Ratio'] = df['Body_Size'] / (df['Upper_Shadow'] + df['Lower_Shadow'] + 0.001)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create market microstructure features\"\"\"\n",
        "        # Bid-Ask Spread Proxy (using High-Low as proxy)\n",
        "        df['Spread_Proxy'] = (df['High'] - df['Low']) / df['Close']\n",
        "        \n",
        "        # Amihud Illiquidity Measure\n",
        "        df['Illiquidity'] = abs(df['Close'].pct_change()) / (df['Volume'] + 1)\n",
        "        df['Illiquidity_MA'] = df['Illiquidity'].rolling(20).mean()\n",
        "        \n",
        "        # Roll Measure (serial covariance of price changes)\n",
        "        returns = df['Close'].pct_change()\n",
        "        df['Roll_Measure'] = returns.rolling(20).apply(\n",
        "            lambda x: 2 * np.sqrt(-np.cov(x[:-1], x[1:])[0, 1]) if len(x) > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # Kyle's Lambda (price impact)\n",
        "        df['Kyle_Lambda'] = abs(returns) / np.log(df['Volume'] + 1)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_momentum_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create momentum indicators\"\"\"\n",
        "        # Rate of Change (ROC)\n",
        "        for period in [5, 10, 20]:\n",
        "            df[f'ROC_{period}'] = ((df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period)) * 100\n",
        "        \n",
        "        # Stochastic Oscillator\n",
        "        for period in [14, 28]:\n",
        "            low_min = df['Low'].rolling(period).min()\n",
        "            high_max = df['High'].rolling(period).max()\n",
        "            df[f'Stoch_{period}'] = ((df['Close'] - low_min) / (high_max - low_min)) * 100\n",
        "            df[f'Stoch_{period}_MA3'] = df[f'Stoch_{period}'].rolling(3).mean()\n",
        "        \n",
        "        # Williams %R\n",
        "        df['Williams_R'] = ((df['High'].rolling(14).max() - df['Close']) / \n",
        "                           (df['High'].rolling(14).max() - df['Low'].rolling(14).min())) * -100\n",
        "        \n",
        "        # Commodity Channel Index (CCI)\n",
        "        typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "        sma = typical_price.rolling(20).mean()\n",
        "        mean_deviation = typical_price.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
        "        df['CCI'] = (typical_price - sma) / (0.015 * mean_deviation)\n",
        "        \n",
        "        # Ultimate Oscillator\n",
        "        bp = df['Close'] - df[['Low', 'Close']].shift(1).min(axis=1)\n",
        "        tr = df[['High', 'Close']].shift(1).max(axis=1) - df[['Low', 'Close']].shift(1).min(axis=1)\n",
        "        \n",
        "        avg7 = (bp.rolling(7).sum() / tr.rolling(7).sum()) * 100\n",
        "        avg14 = (bp.rolling(14).sum() / tr.rolling(14).sum()) * 100\n",
        "        avg28 = (bp.rolling(28).sum() / tr.rolling(28).sum()) * 100\n",
        "        \n",
        "        df['Ultimate_Oscillator'] = (avg7 * 4 + avg14 * 2 + avg28) / 7\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create statistical features\"\"\"\n",
        "        returns = df['Close'].pct_change()\n",
        "        \n",
        "        # Rolling statistics\n",
        "        for period in [5, 10, 20, 60]:\n",
        "            df[f'Return_Mean_{period}'] = returns.rolling(period).mean()\n",
        "            df[f'Return_Std_{period}'] = returns.rolling(period).std()\n",
        "            df[f'Return_Skew_{period}'] = returns.rolling(period).skew()\n",
        "            df[f'Return_Kurt_{period}'] = returns.rolling(period).kurt()\n",
        "            \n",
        "            # Z-score\n",
        "            df[f'Z_Score_{period}'] = (df['Close'] - df['Close'].rolling(period).mean()) / df['Close'].rolling(period).std()\n",
        "        \n",
        "        # Hurst Exponent (trend strength)\n",
        "        def hurst_exponent(ts, lag=20):\n",
        "            lags = range(2, lag)\n",
        "            tau = [np.std(np.subtract(ts[lag:], ts[:-lag])) for lag in lags]\n",
        "            poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
        "            return poly[0] * 2.0\n",
        "        \n",
        "        df['Hurst_Exponent'] = df['Close'].rolling(60).apply(\n",
        "            lambda x: hurst_exponent(x.values) if len(x) >= 20 else 0.5\n",
        "        )\n",
        "        \n",
        "        # Autocorrelation\n",
        "        for lag in [1, 5, 10]:\n",
        "            df[f'Autocorr_{lag}'] = returns.rolling(20).apply(\n",
        "                lambda x: x.autocorr(lag=lag) if len(x) > lag else 0\n",
        "            )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def create_interaction_features(self, df: pd.DataFrame, \n",
        "                                  feature_cols: List[str], \n",
        "                                  max_interactions: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create interaction features between important features\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame with features\n",
        "            feature_cols: List of feature column names\n",
        "            max_interactions: Maximum number of interaction features to create\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with interaction features\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating interaction features from {len(feature_cols)} features\")\n",
        "        \n",
        "        # Select most important features for interactions\n",
        "        if len(feature_cols) > 10:\n",
        "            # Use correlation with target to select top features\n",
        "            if 'Target' in df.columns:\n",
        "                correlations = df[feature_cols].corrwith(df['Target']).abs()\n",
        "                top_features = correlations.nlargest(10).index.tolist()\n",
        "            else:\n",
        "                top_features = feature_cols[:10]\n",
        "        else:\n",
        "            top_features = feature_cols\n",
        "        \n",
        "        # Create polynomial interactions\n",
        "        interaction_count = 0\n",
        "        for i, feat1 in enumerate(top_features):\n",
        "            for feat2 in top_features[i+1:]:\n",
        "                if interaction_count >= max_interactions:\n",
        "                    break\n",
        "                \n",
        "                # Multiplication interaction\n",
        "                df[f'{feat1}_X_{feat2}'] = df[feat1] * df[feat2]\n",
        "                \n",
        "                # Ratio interaction (with small constant to avoid division by zero)\n",
        "                df[f'{feat1}_DIV_{feat2}'] = df[feat1] / (df[feat2] + 0.001)\n",
        "                \n",
        "                interaction_count += 2\n",
        "        \n",
        "        logger.info(f\"Created {interaction_count} interaction features\")\n",
        "        return df\n",
        "    \n",
        "    def select_features(self, X: pd.DataFrame, y: pd.Series, \n",
        "                       method: str = 'mutual_info', \n",
        "                       k: int = 30) -> Tuple[pd.DataFrame, List[str]]:\n",
        "        \"\"\"\n",
        "        Select top k features using specified method\n",
        "        \n",
        "        Args:\n",
        "            X: Feature DataFrame\n",
        "            y: Target Series\n",
        "            method: Selection method ('mutual_info', 'correlation', 'rfe')\n",
        "            k: Number of features to select\n",
        "            \n",
        "        Returns:\n",
        "            Selected features DataFrame and list of selected feature names\n",
        "        \"\"\"\n",
        "        logger.info(f\"Selecting top {k} features using {method} method\")\n",
        "        \n",
        "        # Remove any infinite or NaN values\n",
        "        X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        \n",
        "        if method == 'mutual_info':\n",
        "            selector = SelectKBest(score_func=mutual_info_regression, k=min(k, X_clean.shape[1]))\n",
        "            X_selected = selector.fit_transform(X_clean, y)\n",
        "            selected_features = X_clean.columns[selector.get_support()].tolist()\n",
        "            \n",
        "        elif method == 'correlation':\n",
        "            correlations = X_clean.corrwith(y).abs()\n",
        "            selected_features = correlations.nlargest(min(k, len(correlations))).index.tolist()\n",
        "            X_selected = X_clean[selected_features]\n",
        "            \n",
        "        elif method == 'rfe':\n",
        "            estimator = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "            selector = RFE(estimator, n_features_to_select=min(k, X_clean.shape[1]))\n",
        "            X_selected = selector.fit_transform(X_clean, y)\n",
        "            selected_features = X_clean.columns[selector.support_].tolist()\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown selection method: {method}\")\n",
        "        \n",
        "        self.selected_features = selected_features\n",
        "        logger.info(f\"Selected {len(selected_features)} features\")\n",
        "        \n",
        "        return pd.DataFrame(X_selected, columns=selected_features, index=X.index), selected_features\n",
        "    \n",
        "    def apply_pca(self, X: pd.DataFrame, n_components: float = 0.95) -> Tuple[pd.DataFrame, PCA]:\n",
        "        \"\"\"\n",
        "        Apply PCA for dimensionality reduction\n",
        "        \n",
        "        Args:\n",
        "            X: Feature DataFrame\n",
        "            n_components: Number of components or variance to preserve\n",
        "            \n",
        "        Returns:\n",
        "            Transformed DataFrame and fitted PCA object\n",
        "        \"\"\"\n",
        "        logger.info(f\"Applying PCA with n_components={n_components}\")\n",
        "        \n",
        "        # Scale features before PCA\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        # Apply PCA\n",
        "        self.pca = PCA(n_components=n_components)\n",
        "        X_pca = self.pca.fit_transform(X_scaled)\n",
        "        \n",
        "        # Create DataFrame with PCA components\n",
        "        pca_cols = [f'PCA_{i+1}' for i in range(X_pca.shape[1])]\n",
        "        X_pca_df = pd.DataFrame(X_pca, columns=pca_cols, index=X.index)\n",
        "        \n",
        "        logger.info(f\"PCA reduced {X.shape[1]} features to {X_pca.shape[1]} components\")\n",
        "        logger.info(f\"Explained variance ratio: {self.pca.explained_variance_ratio_.sum():.2%}\")\n",
        "        \n",
        "        return X_pca_df, self.pca\n",
        "    \n",
        "    def scale_features(self, X: pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Scale features using the configured scaler\n",
        "        \n",
        "        Args:\n",
        "            X: Feature DataFrame\n",
        "            fit: Whether to fit the scaler (True for training, False for testing)\n",
        "            \n",
        "        Returns:\n",
        "            Scaled DataFrame\n",
        "        \"\"\"\n",
        "        if fit:\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "            logger.info(f\"Fitted and transformed features using {self.scaling_method} scaler\")\n",
        "        else:\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "            logger.info(f\"Transformed features using {self.scaling_method} scaler\")\n",
        "        \n",
        "        return pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "    \n",
        "    def prepare_sequences_lstm(self, X: pd.DataFrame, y: pd.Series, \n",
        "                               sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare sequences for LSTM model\n",
        "        \n",
        "        Args:\n",
        "            X: Feature DataFrame\n",
        "            y: Target Series\n",
        "            sequence_length: Number of time steps to look back\n",
        "            \n",
        "        Returns:\n",
        "            3D array for LSTM input and corresponding targets\n",
        "        \"\"\"\n",
        "        logger.info(f\"Preparing LSTM sequences with length {sequence_length}\")\n",
        "        \n",
        "        X_sequences = []\n",
        "        y_sequences = []\n",
        "        \n",
        "        for i in range(sequence_length, len(X)):\n",
        "            X_sequences.append(X.iloc[i-sequence_length:i].values)\n",
        "            y_sequences.append(y.iloc[i])\n",
        "        \n",
        "        X_sequences = np.array(X_sequences)\n",
        "        y_sequences = np.array(y_sequences)\n",
        "        \n",
        "        logger.info(f\"Created {len(X_sequences)} sequences of shape {X_sequences.shape}\")\n",
        "        \n",
        "        return X_sequences, y_sequences\n",
        "    \n",
        "    def create_lagged_features(self, df: pd.DataFrame, \n",
        "                               columns: List[str], \n",
        "                               lags: List[int] = [1, 2, 3, 5, 7]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create lagged features for specified columns\n",
        "        \n",
        "        Args:\n",
        "            df: Input DataFrame\n",
        "            columns: Columns to create lags for\n",
        "            lags: List of lag periods\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with lagged features\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating lagged features for {len(columns)} columns\")\n",
        "        \n",
        "        for col in columns:\n",
        "            if col in df.columns:\n",
        "                for lag in lags:\n",
        "                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def create_rolling_features(self, df: pd.DataFrame, \n",
        "                               columns: List[str], \n",
        "                               windows: List[int] = [5, 10, 20]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create rolling window features\n",
        "        \n",
        "        Args:\n",
        "            df: Input DataFrame\n",
        "            columns: Columns to create rolling features for\n",
        "            windows: List of window sizes\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with rolling features\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating rolling features for {len(columns)} columns\")\n",
        "        \n",
        "        for col in columns:\n",
        "            if col in df.columns:\n",
        "                for window in windows:\n",
        "                    df[f'{col}_roll_mean_{window}'] = df[col].rolling(window).mean()\n",
        "                    df[f'{col}_roll_std_{window}'] = df[col].rolling(window).std()\n",
        "                    df[f'{col}_roll_min_{window}'] = df[col].rolling(window).min()\n",
        "                    df[f'{col}_roll_max_{window}'] = df[col].rolling(window).max()\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def create_target_features(self, df: pd.DataFrame, \n",
        "                              target_col: str = 'Close',\n",
        "                              horizons: List[int] = [1, 5, 10]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create different target variables for various prediction horizons\n",
        "        \n",
        "        Args:\n",
        "            df: Input DataFrame\n",
        "            target_col: Column to create targets from\n",
        "            horizons: List of prediction horizons (days ahead)\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with target features\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating target features for horizons {horizons}\")\n",
        "        \n",
        "        for horizon in horizons:\n",
        "            # Future return\n",
        "            df[f'Target_Return_{horizon}d'] = (\n",
        "                df[target_col].shift(-horizon) / df[target_col] - 1\n",
        "            )\n",
        "            \n",
        "            # Binary classification target (up/down)\n",
        "            df[f'Target_Direction_{horizon}d'] = (\n",
        "                df[f'Target_Return_{horizon}d'] > 0\n",
        "            ).astype(int)\n",
        "            \n",
        "            # Multi-class target (strong down, down, neutral, up, strong up)\n",
        "            conditions = [\n",
        "                df[f'Target_Return_{horizon}d'] < -0.02,\n",
        "                df[f'Target_Return_{horizon}d'] < -0.005,\n",
        "                df[f'Target_Return_{horizon}d'] < 0.005,\n",
        "                df[f'Target_Return_{horizon}d'] < 0.02\n",
        "            ]\n",
        "            choices = [0, 1, 2, 3]\n",
        "            df[f'Target_Class_{horizon}d'] = np.select(conditions, choices, default=4)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def remove_multicollinearity(self, X: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove highly correlated features\n",
        "        \n",
        "        Args:\n",
        "            X: Feature DataFrame\n",
        "            threshold: Correlation threshold\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with reduced multicollinearity\n",
        "        \"\"\"\n",
        "        logger.info(f\"Removing features with correlation > {threshold}\")\n",
        "        \n",
        "        corr_matrix = X.corr().abs()\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "        \n",
        "        to_drop = [column for column in upper_tri.columns \n",
        "                  if any(upper_tri[column] > threshold)]\n",
        "        \n",
        "        X_reduced = X.drop(columns=to_drop)\n",
        "        \n",
        "        logger.info(f\"Removed {len(to_drop)} highly correlated features\")\n",
        "        \n",
        "        return X_reduced\n",
        "    \n",
        "    def engineer_all_features(self, df: pd.DataFrame, \n",
        "                             target_col: str = 'Close',\n",
        "                             sequence_length: int = 60) -> Dict:\n",
        "        \"\"\"\n",
        "        Complete feature engineering pipeline\n",
        "        \n",
        "        Args:\n",
        "            df: Raw DataFrame with OHLCV data\n",
        "            target_col: Column to predict\n",
        "            sequence_length: Sequence length for LSTM\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with processed features and metadata\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting complete feature engineering pipeline\")\n",
        "        \n",
        "        # Create advanced features\n",
        "        df = self.create_advanced_features(df)\n",
        "        \n",
        "        # Create lagged and rolling features\n",
        "        price_cols = ['Close', 'Volume', 'Returns']\n",
        "        df = self.create_lagged_features(df, price_cols, lags=[1, 2, 3, 5, 7, 10])\n",
        "        df = self.create_rolling_features(df, price_cols, windows=[5, 10, 20, 30])\n",
        "        \n",
        "        # Create targets\n",
        "        df = self.create_target_features(df, target_col=target_col, horizons=[1, 5, 10])\n",
        "        \n",
        "        # Drop rows with NaN\n",
        "        df = df.dropna()\n",
        "        \n",
        "        # Separate features and targets\n",
        "        feature_cols = [col for col in df.columns \n",
        "                       if not col.startswith('Target_') and col != 'Date']\n",
        "        target_cols = [col for col in df.columns if col.startswith('Target_')]\n",
        "        \n",
        "        X = df[feature_cols]\n",
        "        y = df['Target_Return_1d']  # Default to 1-day return prediction\n",
        "        \n",
        "        # Remove multicollinearity\n",
        "        X = self.remove_multicollinearity(X, threshold=0.95)\n",
        "        \n",
        "        # Select top features\n",
        "        X_selected, selected_features = self.select_features(X, y, method='mutual_info', k=50)\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scale_features(X_selected, fit=True)\n",
        "        \n",
        "        # Prepare LSTM sequences if needed\n",
        "        X_lstm, y_lstm = self.prepare_sequences_lstm(X_scaled, y, sequence_length)\n",
        "        \n",
        "        # Store feature names\n",
        "        self.feature_names = X_scaled.columns.tolist()\n",
        "        \n",
        "        result = {\n",
        "            'features': X_scaled,\n",
        "            'features_lstm': X_lstm,\n",
        "            'target': y,\n",
        "            'target_lstm': y_lstm,\n",
        "            'feature_names': self.feature_names,\n",
        "            'selected_features': selected_features,\n",
        "            'full_dataframe': df,\n",
        "            'scaler': self.scaler\n",
        "        }\n",
        "        \n",
        "        logger.info(\"Feature engineering pipeline complete\")\n",
        "        logger.info(f\"Final feature shape: {X_scaled.shape}\")\n",
        "        logger.info(f\"LSTM sequence shape: {X_lstm.shape}\")\n",
        "        \n",
        "        return result\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample data\n",
        "    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Open': 100 + np.random.randn(len(dates)).cumsum(),\n",
        "        'High': 102 + np.random.randn(len(dates)).cumsum(),\n",
        "        'Low': 98 + np.random.randn(len(dates)).cumsum(),\n",
        "        'Close': 100 + np.random.randn(len(dates)).cumsum(),\n",
        "        'Volume': np.random.randint(1000000, 10000000, len(dates))\n",
        "    })\n",
        "    \n",
        "    # Initialize feature engineer\n",
        "    engineer = FeatureEngineer(scaling_method='robust')\n",
        "    \n",
        "    # Engineer all features\n",
        "    result = engineer.engineer_all_features(df)\n",
        "    \n",
        "    print(\"\\nFeature Engineering Results:\")\n",
        "    print(f\"Number of features: {len(result['feature_names'])}\")\n",
        "    print(f\"Feature shape: {result['features'].shape}\")\n",
        "    print(f\"LSTM sequence shape: {result['features_lstm'].shape}\")\n",
        "    print(f\"\\nTop 10 selected features:\")\n",
        "    print(result['selected_features'][:10])\n",
        "    print(f\"\\nTarget shape: {result['target'].shape}\")\n",
        "    print(f\"Target LSTM shape: {result['target_lstm'].shape}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}