{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Machine Learning Models Module\n",
        "Contains implementations of various ML models for stock price prediction\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, Optional, Any\n",
        "import logging\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TensorFlow/Keras imports (with error handling)\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.models import Sequential, load_model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TF_AVAILABLE = False\n",
        "    logging.warning(\"TensorFlow not available. LSTM models will not work.\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class StockPredictionModels:\n",
        "    \"\"\"Handles training and prediction for various ML models\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict = None):\n",
        "        \"\"\"\n",
        "        Initialize the model trainer\n",
        "\n",
        "        Args:\n",
        "            config: Configuration dictionary for models\n",
        "        \"\"\"\n",
        "        self.config = config or {}\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metrics = {}\n",
        "        self.model_dir = 'models'\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "    def train_linear_regression(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                                X_val: np.ndarray = None, y_val: np.ndarray = None,\n",
        "                                model_type: str = 'linear') -> LinearRegression:\n",
        "        \"\"\"\n",
        "        Train linear regression model\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features\n",
        "            y_val: Validation targets\n",
        "            model_type: Type of linear model ('linear', 'ridge', 'lasso')\n",
        "\n",
        "        Returns:\n",
        "            Trained model\n",
        "        \"\"\"\n",
        "        logger.info(f\"Training {model_type} regression model\")\n",
        "\n",
        "        # Select model type\n",
        "        if model_type == 'ridge':\n",
        "            model = Ridge(alpha=self.config.get('ridge_alpha', 1.0))\n",
        "        elif model_type == 'lasso':\n",
        "            model = Lasso(alpha=self.config.get('lasso_alpha', 1.0))\n",
        "        else:\n",
        "            model = LinearRegression()\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Store model\n",
        "        self.models['linear_regression'] = model\n",
        "\n",
        "        # Evaluate if validation data provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            self.metrics['linear_regression'] = {\n",
        "                'train': self.calculate_metrics(y_train, train_pred),\n",
        "                'val': self.calculate_metrics(y_val, val_pred)\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Linear Regression - Train RMSE: {self.metrics['linear_regression']['train']['rmse']:.4f}\")\n",
        "            logger.info(f\"Linear Regression - Val RMSE: {self.metrics['linear_regression']['val']['rmse']:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_random_forest(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                           X_val: np.ndarray = None, y_val: np.ndarray = None) -> RandomForestRegressor:\n",
        "        \"\"\"\n",
        "        Train Random Forest model\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features\n",
        "            y_val: Validation targets\n",
        "\n",
        "        Returns:\n",
        "            Trained model\n",
        "        \"\"\"\n",
        "        logger.info(\"Training Random Forest model\")\n",
        "\n",
        "        # Get config or use defaults\n",
        "        rf_config = self.config.get('random_forest', {})\n",
        "\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=rf_config.get('n_estimators', 100),\n",
        "            max_depth=rf_config.get('max_depth', 15),\n",
        "            min_samples_split=rf_config.get('min_samples_split', 5),\n",
        "            min_samples_leaf=rf_config.get('min_samples_leaf', 2),\n",
        "            max_features=rf_config.get('max_features', 'sqrt'),\n",
        "            random_state=rf_config.get('random_state', 42),\n",
        "            n_jobs=rf_config.get('n_jobs', -1)\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Store model\n",
        "        self.models['random_forest'] = model\n",
        "\n",
        "        # Evaluate if validation data provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            self.metrics['random_forest'] = {\n",
        "                'train': self.calculate_metrics(y_train, train_pred),\n",
        "                'val': self.calculate_metrics(y_val, val_pred)\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Random Forest - Train RMSE: {self.metrics['random_forest']['train']['rmse']:.4f}\")\n",
        "            logger.info(f\"Random Forest - Val RMSE: {self.metrics['random_forest']['val']['rmse']:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_gradient_boosting(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                               X_val: np.ndarray = None, y_val: np.ndarray = None) -> GradientBoostingRegressor:\n",
        "        \"\"\"\n",
        "        Train Gradient Boosting model\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features\n",
        "            y_val: Validation targets\n",
        "\n",
        "        Returns:\n",
        "            Trained model\n",
        "        \"\"\"\n",
        "        logger.info(\"Training Gradient Boosting model\")\n",
        "\n",
        "        # Get config or use defaults\n",
        "        gb_config = self.config.get('gradient_boosting', {})\n",
        "\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=gb_config.get('n_estimators', 100),\n",
        "            learning_rate=gb_config.get('learning_rate', 0.1),\n",
        "            max_depth=gb_config.get('max_depth', 5),\n",
        "            min_samples_split=gb_config.get('min_samples_split', 5),\n",
        "            min_samples_leaf=gb_config.get('min_samples_leaf', 2),\n",
        "            subsample=gb_config.get('subsample', 0.8),\n",
        "            random_state=gb_config.get('random_state', 42)\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Store model\n",
        "        self.models['gradient_boosting'] = model\n",
        "\n",
        "        # Evaluate if validation data provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            self.metrics['gradient_boosting'] = {\n",
        "                'train': self.calculate_metrics(y_train, train_pred),\n",
        "                'val': self.calculate_metrics(y_val, val_pred)\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Gradient Boosting - Train RMSE: {self.metrics['gradient_boosting']['train']['rmse']:.4f}\")\n",
        "            logger.info(f\"Gradient Boosting - Val RMSE: {self.metrics['gradient_boosting']['val']['rmse']:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                  X_val: np.ndarray = None, y_val: np.ndarray = None) -> Optional[Any]:\n",
        "        \"\"\"\n",
        "        Train LSTM model\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features (3D array for LSTM)\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features\n",
        "            y_val: Validation targets\n",
        "\n",
        "        Returns:\n",
        "            Trained model or None if TensorFlow not available\n",
        "        \"\"\"\n",
        "        if not TF_AVAILABLE:\n",
        "            logger.error(\"TensorFlow not available. Cannot train LSTM model.\")\n",
        "            return None\n",
        "\n",
        "        logger.info(\"Training LSTM model\")\n",
        "\n",
        "        # Get config or use defaults\n",
        "        lstm_config = self.config.get('lstm', {})\n",
        "\n",
        "        # Build model\n",
        "        model = Sequential()\n",
        "\n",
        "        # LSTM layers\n",
        "        units = lstm_config.get('units', [128, 64, 32])\n",
        "        dropout = lstm_config.get('dropout', 0.2)\n",
        "\n",
        "        for i, unit in enumerate(units):\n",
        "            if i == 0:\n",
        "                model.add(LSTM(unit, return_sequences=(i < len(units) - 1),\n",
        "                              input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "            else:\n",
        "                model.add(LSTM(unit, return_sequences=(i < len(units) - 1)))\n",
        "\n",
        "            model.add(Dropout(dropout))\n",
        "\n",
        "        # Dense layers\n",
        "        dense_units = lstm_config.get('dense_units', [32, 16])\n",
        "        for unit in dense_units:\n",
        "            model.add(Dense(unit, activation='relu'))\n",
        "            model.add(Dropout(dropout / 2))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = Adam(learning_rate=lstm_config.get('learning_rate', 0.001))\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                patience=lstm_config.get('early_stopping_patience', 15),\n",
        "                restore_best_weights=True,\n",
        "                monitor='val_loss' if X_val is not None else 'loss'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                patience=lstm_config.get('reduce_lr_patience', 10),\n",
        "                factor=lstm_config.get('reduce_lr_factor', 0.5),\n",
        "                min_lr=lstm_config.get('min_lr', 0.00001),\n",
        "                monitor='val_loss' if X_val is not None else 'loss'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Add model checkpoint\n",
        "        checkpoint_path = os.path.join(self.model_dir, 'lstm_best.h5')\n",
        "        callbacks.append(\n",
        "            ModelCheckpoint(\n",
        "                checkpoint_path,\n",
        "                save_best_only=True,\n",
        "                monitor='val_loss' if X_val is not None else 'loss'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        validation_data = (X_val, y_val) if X_val is not None else None\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=lstm_config.get('epochs', 100),\n",
        "            batch_size=lstm_config.get('batch_size', 32),\n",
        "            validation_data=validation_data,\n",
        "            callbacks=callbacks,\n",
        "            verbose=lstm_config.get('verbose', 1)\n",
        "        )\n",
        "\n",
        "        # Store model and history\n",
        "        self.models['lstm'] = model\n",
        "        self.models['lstm_history'] = history.history\n",
        "\n",
        "        # Evaluate if validation data provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            train_pred = model.predict(X_train).flatten()\n",
        "            val_pred = model.predict(X_val).flatten()\n",
        "\n",
        "            self.metrics['lstm'] = {\n",
        "                'train': self.calculate_metrics(y_train, train_pred),\n",
        "                'val': self.calculate_metrics(y_val, val_pred)\n",
        "            }\n",
        "\n",
        "            logger.info(f\"LSTM - Train RMSE: {self.metrics['lstm']['train']['rmse']:.4f}\")\n",
        "            logger.info(f\"LSTM - Val RMSE: {self.metrics['lstm']['val']['rmse']:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_ensemble(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                      X_val: np.ndarray = None, y_val: np.ndarray = None,\n",
        "                      models: list = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Train ensemble of models\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features\n",
        "            y_val: Validation targets\n",
        "            models: List of model names to include in ensemble\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of trained models\n",
        "        \"\"\"\n",
        "        logger.info(\"Training ensemble models\")\n",
        "\n",
        "        if models is None:\n",
        "            models = ['linear_regression', 'random_forest', 'gradient_boosting']\n",
        "\n",
        "        ensemble_models = {}\n",
        "\n",
        "        # Train individual models\n",
        "        if 'linear_regression' in models:\n",
        "            ensemble_models['linear_regression'] = self.train_linear_regression(\n",
        "                X_train, y_train, X_val, y_val\n",
        "            )\n",
        "\n",
        "        if 'random_forest' in models:\n",
        "            ensemble_models['random_forest'] = self.train_random_forest(\n",
        "                X_train, y_train, X_val, y_val\n",
        "            )\n",
        "\n",
        "        if 'gradient_boosting' in models:\n",
        "            ensemble_models['gradient_boosting'] = self.train_gradient_boosting(\n",
        "                X_train, y_train, X_val, y_val\n",
        "            )\n",
        "\n",
        "        # Create ensemble predictions\n",
        "        if X_val is not None and y_val is not None:\n",
        "            ensemble_preds = []\n",
        "            weights = []\n",
        "\n",
        "            for model_name, model in ensemble_models.items():\n",
        "                if model_name in self.models:\n",
        "                    pred = self.predict(model_name, X_val)\n",
        "                    ensemble_preds.append(pred)\n",
        "\n",
        "                    # Use inverse RMSE as weight\n",
        "                    rmse = self.metrics[model_name]['val']['rmse']\n",
        "                    weights.append(1 / rmse if rmse > 0 else 0)\n",
        "\n",
        "            # Normalize weights\n",
        "            weights = np.array(weights)\n",
        "            weights = weights / weights.sum()\n",
        "\n",
        "            # Weighted average ensemble\n",
        "            ensemble_pred = np.average(ensemble_preds, axis=0, weights=weights)\n",
        "\n",
        "            self.metrics['ensemble'] = {\n",
        "                'val': self.calculate_metrics(y_val, ensemble_pred),\n",
        "                'weights': dict(zip(ensemble_models.keys(), weights))\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Ensemble - Val RMSE: {self.metrics['ensemble']['val']['rmse']:.4f}\")\n",
        "            logger.info(f\"Ensemble weights: {self.metrics['ensemble']['weights']}\")\n",
        "\n",
        "        self.models['ensemble'] = ensemble_models\n",
        "        return ensemble_models\n",
        "\n",
        "    def predict(self, model_name: str, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using a trained model\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model\n",
        "            X: Features for prediction\n",
        "\n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not found. Available models: {list(self.models.keys())}\")\n",
        "\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        if model_name == 'ensemble':\n",
        "            # Ensemble prediction\n",
        "            predictions = []\n",
        "            weights = self.metrics['ensemble'].get('weights', {})\n",
        "\n",
        "            for sub_model_name, sub_model in model.items():\n",
        "                pred = sub_model.predict(X)\n",
        "                weight = weights.get(sub_model_name, 1.0 / len(model))\n",
        "                predictions.append(pred * weight)\n",
        "\n",
        "            return np.sum(predictions, axis=0)\n",
        "\n",
        "        elif model_name == 'lstm' and TF_AVAILABLE:\n",
        "            return model.predict(X).flatten()\n",
        "\n",
        "        else:\n",
        "            return model.predict(X)\n",
        "\n",
        "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate various metrics for model evaluation\n",
        "\n",
        "        Args:\n",
        "            y_true: True values\n",
        "            y_pred: Predicted values\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of metrics\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Regression metrics\n",
        "        metrics['mse'] = mean_squared_error(y_true, y_pred)\n",
        "        metrics['rmse'] = np.sqrt(metrics['mse'])\n",
        "        metrics['mae'] = mean_absolute_error(y_true, y_pred)\n",
        "        metrics['r2'] = r2_score(y_true, y_pred)\n",
        "\n",
        "        # MAPE (Mean Absolute Percentage Error)\n",
        "        mask = y_true != 0\n",
        "        if np.any(mask):\n",
        "            metrics['mape'] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "        else:\n",
        "            metrics['mape'] = np.inf\n",
        "\n",
        "        # Directional Accuracy\n",
        "        if len(y_true) > 1:\n",
        "            true_direction = np.diff(y_true) > 0\n",
        "            pred_direction = np.diff(y_pred) > 0\n",
        "            metrics['directional_accuracy'] = np.mean(true_direction == pred_direction)\n",
        "        else:\n",
        "            metrics['directional_accuracy'] = 0\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def cross_validate(self, X: np.ndarray, y: np.ndarray,\n",
        "                      model_type: str = 'random_forest',\n",
        "                      n_splits: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform time series cross-validation\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Targets\n",
        "            model_type: Type of model to validate\n",
        "            n_splits: Number of CV splits\n",
        "\n",
        "        Returns:\n",
        "            Cross-validation results\n",
        "        \"\"\"\n",
        "        logger.info(f\"Performing {n_splits}-fold cross-validation for {model_type}\")\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "        cv_metrics = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Train model based on type\n",
        "            if model_type == 'linear_regression':\n",
        "                self.train_linear_regression(X_train, y_train, X_val, y_val)\n",
        "            elif model_type == 'random_forest':\n",
        "                self.train_random_forest(X_train, y_train, X_val, y_val)\n",
        "            elif model_type == 'gradient_boosting':\n",
        "                self.train_gradient_boosting(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Get metrics\n",
        "            fold_metrics = self.metrics[model_type]['val']\n",
        "            cv_metrics.append(fold_metrics)\n",
        "\n",
        "            logger.info(f\"Fold {fold + 1} - RMSE: {fold_metrics['rmse']:.4f}\")\n",
        "\n",
        "        # Aggregate metrics\n",
        "        aggregated = {}\n",
        "        for metric_name in cv_metrics[0].keys():\n",
        "            values = [m[metric_name] for m in cv_metrics]\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'values': values\n",
        "            }\n",
        "\n",
        "        logger.info(f\"CV Average RMSE: {aggregated['rmse']['mean']:.4f} (+/-{aggregated['rmse']['std']:.4f})\")\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                            model_type: str = 'random_forest',\n",
        "                            param_grid: Dict = None, cv: int = 3) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            model_type: Type of model\n",
        "            param_grid: Parameter grid for search\n",
        "            cv: Number of CV folds\n",
        "\n",
        "        Returns:\n",
        "            Best parameters and model\n",
        "        \"\"\"\n",
        "        logger.info(f\"Performing hyperparameter tuning for {model_type}\")\n",
        "\n",
        "        # Default parameter grids\n",
        "        if param_grid is None:\n",
        "            if model_type == 'random_forest':\n",
        "                param_grid = {\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "                    'max_depth': [10, 15, 20, None],\n",
        "                    'min_samples_split': [2, 5, 10],\n",
        "                    'min_samples_leaf': [1, 2, 4]\n",
        "                }\n",
        "            elif model_type == 'gradient_boosting':\n",
        "                param_grid = {\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'max_depth': [3, 5, 7],\n",
        "                    'subsample': [0.8, 0.9, 1.0]\n",
        "                }\n",
        "            else:\n",
        "                raise ValueError(f\"No default param_grid for {model_type}\")\n",
        "\n",
        "        # Select base model\n",
        "        if model_type == 'random_forest':\n",
        "            base_model = RandomForestRegressor(random_state=42)\n",
        "        elif model_type == 'gradient_boosting':\n",
        "            base_model = GradientBoostingRegressor(random_state=42)\n",
        "        else:\n",
        "            raise ValueError(f\"Hyperparameter tuning not supported for {model_type}\")\n",
        "\n",
        "        # Grid search with time series CV\n",
        "        tscv = TimeSeriesSplit(n_splits=cv)\n",
        "        grid_search = GridSearchCV(\n",
        "            base_model, param_grid, cv=tscv,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        # Fit grid search\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Store best model\n",
        "        self.models[f'{model_type}_tuned'] = grid_search.best_estimator_\n",
        "\n",
        "        results = {\n",
        "            'best_params': grid_search.best_params_,\n",
        "            'best_score': -grid_search.best_score_,\n",
        "            'cv_results': grid_search.cv_results_\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Best parameters: {results['best_params']}\")\n",
        "        logger.info(f\"Best CV RMSE: {np.sqrt(results['best_score']):.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_model(self, model_name: str, filepath: str = None):\n",
        "        \"\"\"\n",
        "        Save a trained model\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model to save\n",
        "            filepath: Path to save the model\n",
        "        \"\"\"\n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not found\")\n",
        "\n",
        "        if filepath is None:\n",
        "            filepath = os.path.join(self.model_dir, f'{model_name}.pkl')\n",
        "\n",
        "        if model_name == 'lstm' and TF_AVAILABLE:\n",
        "            # Save Keras model\n",
        "            self.models[model_name].save(filepath.replace('.pkl', '.h5'))\n",
        "        else:\n",
        "            # Save sklearn model\n",
        "            joblib.dump(self.models[model_name], filepath)\n",
        "\n",
        "        logger.info(f\"Model {model_name} saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, model_name: str, filepath: str = None):\n",
        "        \"\"\"\n",
        "        Load a saved model\n",
        "\n",
        "        Args:\n",
        "            model_name: Name to assign to loaded model\n",
        "            filepath: Path to load the model from\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = os.path.join(self.model_dir, f'{model_name}.pkl')\n",
        "\n",
        "        if model_name == 'lstm' and TF_AVAILABLE:\n",
        "            # Load Keras model\n",
        "            self.models[model_name] = load_model(filepath.replace('.pkl', '.h5'))\n",
        "        else:\n",
        "            # Load sklearn model\n",
        "            self.models[model_name] = joblib.load(filepath)\n",
        "\n",
        "        logger.info(f\"Model {model_name} loaded from {filepath}\")\n",
        "\n",
        "    def get_feature_importance(self, model_name: str = 'random_forest') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get feature importance from tree-based models\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with feature importance\n",
        "        \"\"\"\n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not found\")\n",
        "\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            return pd.DataFrame({\n",
        "                'feature': range(len(importance)),\n",
        "                'importance': importance\n",
        "            }).sort_values('importance', ascending=False)\n",
        "        else:\n",
        "            logger.warning(f\"Model {model_name} does not have feature_importances_\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample data\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    n_features = 20\n",
        "\n",
        "    X_train = np.random.randn(n_samples, n_features)\n",
        "    y_train = np.sum(X_train[:, :5], axis=1) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "    X_test = np.random.randn(200, n_features)\n",
        "    y_test = np.sum(X_test[:, :5], axis=1) + np.random.randn(200) * 0.1\n",
        "\n",
        "    # Initialize trainer\n",
        "    config = {\n",
        "        'random_forest': {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': 10\n",
        "        },\n",
        "        'lstm': {\n",
        "            'units': [64, 32],\n",
        "            'epochs': 10,\n",
        "            'batch_size': 32\n",
        "        }\n",
        "    }\n",
        "\n",
        "    trainer = StockPredictionModels(config)\n",
        "\n",
        "    # Train models\n",
        "    trainer.train_linear_regression(X_train, y_train, X_test, y_test)\n",
        "    trainer.train_random_forest(X_train, y_train, X_test, y_test)\n",
        "    trainer.train_gradient_boosting(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Train ensemble\n",
        "    trainer.train_ensemble(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Display metrics\n",
        "    print(\"\\nModel Performance:\")\n",
        "    for model_name, metrics in trainer.metrics.items():\n",
        "        if 'val' in metrics:\n",
        "            print(f\"{model_name}: RMSE={metrics['val']['rmse']:.4f}, R\ufffd={metrics['val']['r2']:.4f}\")\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_results = trainer.cross_validate(X_train, y_train, 'random_forest', n_splits=3)\n",
        "    print(f\"\\nCross-validation RMSE: {cv_results['rmse']['mean']:.4f} (\ufffd{cv_results['rmse']['std']:.4f})\")\n",
        "\n",
        "    # Save models\n",
        "    trainer.save_model('random_forest')\n",
        "    print(\"\\nModel saved successfully\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}