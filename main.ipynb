{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632e0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Pipeline for Stock Price Prediction Project\n",
    "Orchestrates data collection, feature engineering, model training, and evaluation\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from stocks import StockDataCollector\n",
    "from feature_engineering import FeatureEngineer\n",
    "from models import StockPredictionModels\n",
    "from visualization import StockVisualizer\n",
    "import config\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=config.LOGGING_CONFIG['level'],\n",
    "    format=config.LOGGING_CONFIG['format'],\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOGGING_CONFIG['log_file']),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class StockPredictionPipeline:\n",
    "    \"\"\"Main pipeline for stock prediction workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: str, start_date: str = None, end_date: str = None):\n",
    "        \"\"\"\n",
    "        Initialize pipeline\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date for data collection\n",
    "            end_date: End date for data collection\n",
    "        \"\"\"\n",
    "        self.symbol = symbol\n",
    "        self.start_date = start_date or config.DATA_CONFIG['start_date']\n",
    "        self.end_date = end_date or config.DATA_CONFIG['end_date']\n",
    "        \n",
    "        # Initialize components\n",
    "        self.collector = StockDataCollector(symbol, start_date, end_date)\n",
    "        self.engineer = FeatureEngineer(scaling_method=config.FEATURE_CONFIG['scaling_method'])\n",
    "        self.trainer = StockPredictionModels(config.MODEL_CONFIG)\n",
    "        self.visualizer = StockVisualizer()\n",
    "        \n",
    "        # Storage for results\n",
    "        self.raw_data = None\n",
    "        self.engineered_data = None\n",
    "        self.predictions = {}\n",
    "        self.performance = {}\n",
    "        \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        logger.info(f\"Starting pipeline for {self.symbol}\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        # Step 1: Data Collection\n",
    "        self.collect_data()\n",
    "        \n",
    "        # Step 2: Feature Engineering\n",
    "        self.engineer_features()\n",
    "        \n",
    "        # Step 3: Model Training\n",
    "        self.train_models()\n",
    "        \n",
    "        # Step 4: Evaluation\n",
    "        self.evaluate_models()\n",
    "        \n",
    "        # Step 5: Visualization\n",
    "        self.create_visualizations()\n",
    "        \n",
    "        # Step 6: Save Results\n",
    "        self.save_results()\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        \n",
    "        return self.performance\n",
    "    \n",
    "    def collect_data(self):\n",
    "        \"\"\"Collect and prepare stock data\"\"\"\n",
    "        logger.info(\"Step 1: Collecting stock data...\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch raw data\n",
    "            self.raw_data = self.collector.fetch_stock_data()\n",
    "            logger.info(f\"Collected {len(self.raw_data)} days of data\")\n",
    "            \n",
    "            # Calculate technical indicators\n",
    "            self.collector.calculate_moving_averages()\n",
    "            self.collector.calculate_rsi()\n",
    "            self.collector.calculate_macd()\n",
    "            self.collector.calculate_bollinger_bands()\n",
    "            self.collector.calculate_volatility()\n",
    "            self.collector.calculate_volume_indicators()\n",
    "            self.collector.add_price_features()\n",
    "            \n",
    "            self.raw_data = self.collector.data\n",
    "            \n",
    "            # Save raw data\n",
    "            data_path = os.path.join(config.DATA_DIR, \n",
    "                                     f\"{self.symbol}_raw_data_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "            self.raw_data.to_csv(data_path, index=False)\n",
    "            logger.info(f\"Raw data saved to {data_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data collection: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def engineer_features(self):\n",
    "        \"\"\"Engineer features for model training\"\"\"\n",
    "        logger.info(\"Step 2: Engineering features...\")\n",
    "        \n",
    "        try:\n",
    "            # Create advanced features\n",
    "            self.engineered_data = self.engineer.engineer_all_features(\n",
    "                self.raw_data,\n",
    "                target_col='Close',\n",
    "                sequence_length=config.DATA_CONFIG['sequence_length']\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created {len(self.engineered_data['feature_names'])} features\")\n",
    "            logger.info(f\"Selected top {len(self.engineered_data['selected_features'])} features\")\n",
    "            \n",
    "            # Save feature names\n",
    "            features_path = os.path.join(config.DATA_DIR, \n",
    "                                        f\"{self.symbol}_features_{datetime.now().strftime('%Y%m%d')}.json\")\n",
    "            with open(features_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'all_features': self.engineered_data['feature_names'],\n",
    "                    'selected_features': self.engineered_data['selected_features']\n",
    "                }, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in feature engineering: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        logger.info(\"Step 3: Training models...\")\n",
    "        \n",
    "        # Prepare data splits\n",
    "        features = self.engineered_data['features']\n",
    "        target = self.engineered_data['target']\n",
    "        features_lstm = self.engineered_data['features_lstm']\n",
    "        target_lstm = self.engineered_data['target_lstm']\n",
    "        \n",
    "        # Calculate split indices\n",
    "        train_size = int(len(features) * (1 - config.DATA_CONFIG['test_size']))\n",
    "        val_size = int(train_size * config.DATA_CONFIG['validation_size'])\n",
    "        \n",
    "        # Traditional ML data split\n",
    "        X_train = features[:train_size-val_size].values\n",
    "        y_train = target[:train_size-val_size].values\n",
    "        X_val = features[train_size-val_size:train_size].values\n",
    "        y_val = target[train_size-val_size:train_size].values\n",
    "        X_test = features[train_size:].values\n",
    "        y_test = target[train_size:].values\n",
    "        \n",
    "        # LSTM data split\n",
    "        lstm_train_size = int(len(features_lstm) * (1 - config.DATA_CONFIG['test_size']))\n",
    "        lstm_val_size = int(lstm_train_size * config.DATA_CONFIG['validation_size'])\n",
    "        \n",
    "        X_train_lstm = features_lstm[:lstm_train_size-lstm_val_size]\n",
    "        y_train_lstm = target_lstm[:lstm_train_size-lstm_val_size]\n",
    "        X_val_lstm = features_lstm[lstm_train_size-lstm_val_size:lstm_train_size]\n",
    "        y_val_lstm = target_lstm[lstm_train_size-lstm_val_size:lstm_train_size]\n",
    "        X_test_lstm = features_lstm[lstm_train_size:]\n",
    "        y_test_lstm = target_lstm[lstm_train_size:]\n",
    "        \n",
    "        # Store test data for later evaluation\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_lstm = X_test_lstm\n",
    "        self.y_test_lstm = y_test_lstm\n",
    "        \n",
    "        # Train models\n",
    "        models_to_train = [\n",
    "            ('Linear Regression', 'linear'),\n",
    "            ('Random Forest', 'random_forest'),\n",
    "            ('Gradient Boosting', 'gradient_boosting')\n",
    "        ]\n",
    "        \n",
    "        for model_name, model_type in models_to_train:\n",
    "            try:\n",
    "                logger.info(f\"Training {model_name}...\")\n",
    "                \n",
    "                if model_type == 'linear':\n",
    "                    self.trainer.train_linear_regression(X_train, y_train, X_val, y_val)\n",
    "                elif model_type == 'random_forest':\n",
    "                    self.trainer.train_random_forest(X_train, y_train, X_val, y_val)\n",
    "                elif model_type == 'gradient_boosting':\n",
    "                    self.trainer.train_gradient_boosting(X_train, y_train, X_val, y_val)\n",
    "                \n",
    "                logger.info(f\"{model_name} training completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Train LSTM if enabled\n",
    "        if config.TRAINING_CONFIG.get('train_lstm', True):\n",
    "            try:\n",
    "                logger.info(\"Training LSTM model...\")\n",
    "                self.trainer.train_lstm(X_train_lstm, y_train_lstm, \n",
    "                                       X_val_lstm, y_val_lstm)\n",
    "                logger.info(\"LSTM training completed\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training LSTM: {str(e)}\")\n",
    "        \n",
    "        # Train ensemble\n",
    "        try:\n",
    "            logger.info(\"Training Ensemble model...\")\n",
    "            self.trainer.train_ensemble(X_train, y_train, X_val, y_val)\n",
    "            logger.info(\"Ensemble training completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training Ensemble: {str(e)}\")\n",
    "    \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate all trained models\"\"\"\n",
    "        logger.info(\"Step 4: Evaluating models...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        for model_name in self.trainer.models.keys():\n",
    "            try:\n",
    "                if 'lstm' in model_name.lower() or 'gru' in model_name.lower():\n",
    "                    self.predictions[model_name] = self.trainer.predict(\n",
    "                        model_name, self.X_test_lstm\n",
    "                    )\n",
    "                else:\n",
    "                    self.predictions[model_name] = self.trainer.predict(\n",
    "                        model_name, self.X_test\n",
    "                    )\n",
    "                \n",
    "                logger.info(f\"Generated predictions for {model_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error predicting with {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Calculate performance metrics for each model\n",
    "        comparison_data = []\n",
    "        for model_name, predictions in self.predictions.items():\n",
    "            if 'lstm' in model_name.lower():\n",
    "                metrics = self.trainer.calculate_metrics(self.y_test_lstm, predictions)\n",
    "            else:\n",
    "                metrics = self.trainer.calculate_metrics(self.y_test, predictions)\n",
    "\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'RMSE': metrics['rmse'],\n",
    "                'MAE': metrics['mae'],\n",
    "                'R2': metrics['r2'],\n",
    "                'Directional_Accuracy': metrics['directional_accuracy']\n",
    "            })\n",
    "\n",
    "        self.performance = pd.DataFrame(comparison_data).sort_values('RMSE')\n",
    "\n",
    "        \n",
    "        logger.info(\"\\nModel Performance Summary:\")\n",
    "        logger.info(\"-\" * 50)\n",
    "        print(self.performance.to_string())\n",
    "        \n",
    "        # Identify best model\n",
    "        best_model = self.performance.iloc[0]['Model']\n",
    "        logger.info(f\"\\nBest performing model: {best_model}\")\n",
    "        \n",
    "        # Save model comparison\n",
    "        comparison_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                      f\"{self.symbol}_model_comparison_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "        self.performance.to_csv(comparison_path, index=False)\n",
    "        \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"Create and save visualizations\"\"\"\n",
    "        logger.info(\"Step 5: Creating visualizations...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for visualization\n",
    "            viz_data = {\n",
    "                'stock_data': self.raw_data,\n",
    "                'symbol': self.symbol,\n",
    "                'indicators': ['SMA_20', 'SMA_50', 'SMA_200'],\n",
    "                'predictions': self.predictions,\n",
    "                'actual': self.y_test if len(self.predictions) > 0 else None,\n",
    "                'comparison': self.performance,\n",
    "                'training_history': self.trainer.models.get('lstm_history', {})\n",
    "            }\n",
    "            \n",
    "            # Create visualizations\n",
    "            # Stock price plot\n",
    "            stock_fig = self.visualizer.plot_stock_price(\n",
    "                self.raw_data,\n",
    "                self.symbol,\n",
    "                ['SMA_20', 'SMA_50', 'SMA_200']\n",
    "            )\n",
    "\n",
    "            # Predictions plot\n",
    "            if self.predictions:\n",
    "                predictions_fig = self.visualizer.plot_predictions(\n",
    "                    self.y_test,\n",
    "                    self.predictions\n",
    "                )\n",
    "\n",
    "            # Model comparison\n",
    "            if not self.performance.empty:\n",
    "                comparison_fig = self.visualizer.plot_model_comparison(self.performance)\n",
    "            \n",
    "            logger.info(f\"Visualizations saved to {config.OUTPUT_DIR}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save all results and models\"\"\"\n",
    "        logger.info(\"Step 6: Saving results...\")\n",
    "        \n",
    "        # Save models\n",
    "        for model_name in self.trainer.models.keys():\n",
    "            try:\n",
    "                model_path = os.path.join(config.MODEL_DIR,\n",
    "                                         f\"{self.symbol}_{model_name}_{datetime.now().strftime('%Y%m%d')}\")\n",
    "                self.trainer.save_model(model_name, model_path)\n",
    "                logger.info(f\"Saved {model_name} model\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Save predictions\n",
    "        if self.predictions:\n",
    "            predictions_df = pd.DataFrame(self.predictions)\n",
    "            predictions_df['actual'] = self.y_test[:len(predictions_df)]\n",
    "            predictions_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                           f\"{self.symbol}_predictions_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "            predictions_df.to_csv(predictions_path, index=False)\n",
    "            logger.info(f\"Predictions saved to {predictions_path}\")\n",
    "        \n",
    "        # Save summary report\n",
    "        self.generate_report()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate summary report\"\"\"\n",
    "        report_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                  f\"{self.symbol}_report_{datetime.now().strftime('%Y%m%d')}.txt\")\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"Stock Price Prediction Report\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            f.write(f\"Symbol: {self.symbol}\\n\")\n",
    "            f.write(f\"Date Range: {self.start_date} to {self.end_date}\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Data Statistics:\\n\")\n",
    "            f.write(f\"- Total days: {len(self.raw_data)}\\n\")\n",
    "            f.write(f\"- Features created: {len(self.engineered_data['feature_names'])}\\n\")\n",
    "            f.write(f\"- Features selected: {len(self.engineered_data['selected_features'])}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Model Performance:\\n\")\n",
    "            f.write(self.performance.to_string())\n",
    "            f.write(f\"\\n\\nBest Model: {self.performance.iloc[0]['Model']}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nTop 10 Selected Features:\\n\")\n",
    "            for i, feature in enumerate(self.engineered_data['selected_features'][:10]):\n",
    "                f.write(f\"  {i+1}. {feature}\\n\")\n",
    "\n",
    "        logger.info(f\"Report saved to {report_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the pipeline\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Stock Price Prediction Pipeline')\n",
    "\n",
    "    parser.add_argument('symbol', type=str, help='Stock symbol (e.g., AAPL, GOOGL)')\n",
    "    parser.add_argument('--start-date', type=str, help='Start date (YYYY-MM-DD)')\n",
    "    parser.add_argument('--end-date', type=str, help='End date (YYYY-MM-DD)')\n",
    "    parser.add_argument('--dashboard', action='store_true', help='Launch dashboard after training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = StockPredictionPipeline(\n",
    "        symbol=args.symbol,\n",
    "        start_date=args.start_date,\n",
    "        end_date=args.end_date\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    try:\n",
    "        results = pipeline.run_pipeline()\n",
    "\n",
    "        # Launch dashboard if requested\n",
    "        if args.dashboard:\n",
    "            logger.info(\"Launching dashboard...\")\n",
    "            from dashboard import app\n",
    "            app.run(\n",
    "                debug=config.DASHBOARD_CONFIG['debug'],\n",
    "                host=config.DASHBOARD_CONFIG['host'],\n",
    "                port=config.DASHBOARD_CONFIG['port']\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e3f2ce-1edd-47e0-a39b-da32b0723fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:09:52,793 - main - INFO - Starting pipeline for AAPL\n",
      "2025-11-14 23:09:52,794 - main - INFO - ==================================================\n",
      "2025-11-14 23:09:52,794 - main - INFO - Step 1: Collecting stock data...\n",
      "2025-11-14 23:09:52,794 - stocks - INFO - Fetching data for AAPL from 2021-01-01 to 2024-12-31 (interval=1d)\n",
      "2025-11-14 23:09:53,429 - stocks - INFO - Successfully fetched 1004 rows\n",
      "2025-11-14 23:09:53,430 - main - INFO - Collected 1004 days of data\n",
      "2025-11-14 23:09:53,433 - stocks - INFO - Calculated moving averages for windows: [10, 20, 50, 200]\n",
      "2025-11-14 23:09:53,434 - stocks - INFO - Calculated RSI_14\n",
      "2025-11-14 23:09:53,435 - stocks - INFO - Calculated MACD (12, 26, 9)\n",
      "2025-11-14 23:09:53,437 - stocks - INFO - Calculated Bollinger Bands (window=20, std=2.0)\n",
      "2025-11-14 23:09:53,438 - stocks - INFO - Calculated volatility metrics (window=20)\n",
      "2025-11-14 23:09:53,452 - stocks - INFO - Calculated volume indicators\n",
      "2025-11-14 23:09:53,453 - stocks - INFO - Added price/lag features with lags=[1, 3, 5, 10]\n",
      "2025-11-14 23:09:53,478 - main - INFO - Raw data saved to /Users/charlie/PredictiveStockAnalysis/data/AAPL_raw_data_20251114.csv\n",
      "2025-11-14 23:09:53,479 - main - INFO - Step 2: Engineering features...\n",
      "2025-11-14 23:09:53,479 - feature_engineering - INFO - Starting complete feature engineering pipeline\n",
      "2025-11-14 23:09:53,479 - feature_engineering - INFO - Creating advanced technical features...\n",
      "2025-11-14 23:09:53,846 - feature_engineering - INFO - Created 111 total features\n",
      "2025-11-14 23:09:53,846 - feature_engineering - INFO - Creating lagged features for 3 columns\n",
      "2025-11-14 23:09:53,849 - feature_engineering - INFO - Creating rolling features for 3 columns\n",
      "2025-11-14 23:09:53,854 - feature_engineering - INFO - Creating target features for horizons [1, 5, 10]\n",
      "2025-11-14 23:09:53,860 - feature_engineering - INFO - Removing features with correlation > 0.95\n",
      "2025-11-14 23:09:53,878 - feature_engineering - INFO - Removed 75 highly correlated features\n",
      "2025-11-14 23:09:53,878 - feature_engineering - INFO - Selecting top 50 features using mutual_info method\n",
      "2025-11-14 23:09:53,962 - feature_engineering - INFO - Selected 50 features\n",
      "2025-11-14 23:09:53,966 - feature_engineering - INFO - Fitted and transformed features using robust scaler\n",
      "2025-11-14 23:09:53,966 - feature_engineering - INFO - Preparing LSTM sequences with length 60\n",
      "2025-11-14 23:09:53,970 - feature_engineering - INFO - Created 348 sequences of shape (348, 60, 50)\n",
      "2025-11-14 23:09:53,970 - feature_engineering - INFO - Feature engineering pipeline complete\n",
      "2025-11-14 23:09:53,970 - feature_engineering - INFO - Final feature shape: (408, 50)\n",
      "2025-11-14 23:09:53,970 - feature_engineering - INFO - LSTM sequence shape: (348, 60, 50)\n",
      "2025-11-14 23:09:53,971 - main - INFO - Created 50 features\n",
      "2025-11-14 23:09:53,971 - main - INFO - Selected top 50 features\n",
      "2025-11-14 23:09:53,971 - main - INFO - Step 3: Training models...\n",
      "2025-11-14 23:09:53,972 - main - INFO - Training Linear Regression...\n",
      "2025-11-14 23:09:53,972 - models - INFO - Training linear regression model\n",
      "2025-11-14 23:09:53,990 - models - INFO - Linear Regression - Train RMSE: 0.0157\n",
      "2025-11-14 23:09:54,001 - models - INFO - Linear Regression - Val RMSE: 0.0313\n",
      "2025-11-14 23:09:54,007 - main - INFO - Linear Regression training completed\n",
      "2025-11-14 23:09:54,011 - main - INFO - Training Random Forest...\n",
      "2025-11-14 23:09:54,018 - models - INFO - Training Random Forest model\n",
      "2025-11-14 23:09:54,120 - models - INFO - Random Forest - Train RMSE: 0.0095\n",
      "2025-11-14 23:09:54,121 - models - INFO - Random Forest - Val RMSE: 0.0132\n",
      "2025-11-14 23:09:54,121 - main - INFO - Random Forest training completed\n",
      "2025-11-14 23:09:54,122 - main - INFO - Training Gradient Boosting...\n",
      "2025-11-14 23:09:54,122 - models - INFO - Training Gradient Boosting model\n",
      "2025-11-14 23:09:54,374 - models - INFO - Gradient Boosting - Train RMSE: 0.0015\n",
      "2025-11-14 23:09:54,375 - models - INFO - Gradient Boosting - Val RMSE: 0.0146\n",
      "2025-11-14 23:09:54,375 - main - INFO - Gradient Boosting training completed\n",
      "2025-11-14 23:09:54,375 - main - INFO - Training LSTM model...\n",
      "2025-11-14 23:09:54,375 - models - INFO - Training LSTM model\n",
      "2025-11-14 23:09:54.384815: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2025-11-14 23:09:54.384874: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-11-14 23:09:54.384878: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 14.04 GB\n",
      "2025-11-14 23:09:54.385141: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-14 23:09:54.385151: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:09:55.751357: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0609 - mae: 0.1979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:01,130 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 178ms/step - loss: 0.0643 - mae: 0.2018 - val_loss: 0.0066 - val_mae: 0.0645 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1056 - mae: 0.2539 - val_loss: 0.0074 - val_mae: 0.0782 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1149 - mae: 0.2703 - val_loss: 0.0597 - val_mae: 0.2407 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1607 - mae: 0.3184 - val_loss: 0.0377 - val_mae: 0.1859 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1469 - mae: 0.3012 - val_loss: 0.0116 - val_mae: 0.0950 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2374 - mae: 0.3867 - val_loss: 0.0097 - val_mae: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2469 - mae: 0.3969 - val_loss: 0.0114 - val_mae: 0.0982 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2251 - mae: 0.3663"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:02,928 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2194 - mae: 0.3743 - val_loss: 0.0053 - val_mae: 0.0608 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1887 - mae: 0.3382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:03,191 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1655 - mae: 0.3201 - val_loss: 0.0028 - val_mae: 0.0419 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1685 - mae: 0.3290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:03,467 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1681 - mae: 0.3291 - val_loss: 0.0022 - val_mae: 0.0377 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1891 - mae: 0.3423 - val_loss: 0.0033 - val_mae: 0.0444 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1934 - mae: 0.3612 - val_loss: 0.0028 - val_mae: 0.0441 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1747 - mae: 0.3349 - val_loss: 0.0050 - val_mae: 0.0595 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1960 - mae: 0.3504 - val_loss: 0.0048 - val_mae: 0.0604 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1719 - mae: 0.3308 - val_loss: 0.0026 - val_mae: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1879 - mae: 0.3401 - val_loss: 0.0026 - val_mae: 0.0385 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2107 - mae: 0.3758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:05,177 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2368 - mae: 0.3859 - val_loss: 0.0022 - val_mae: 0.0362 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2224 - mae: 0.3549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:05,456 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2392 - mae: 0.3763 - val_loss: 0.0021 - val_mae: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3251 - mae: 0.4516 - val_loss: 0.0025 - val_mae: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2703 - mae: 0.4031 - val_loss: 0.0036 - val_mae: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2410 - mae: 0.3912 - val_loss: 0.0078 - val_mae: 0.0783 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2402 - mae: 0.3760 - val_loss: 0.0247 - val_mae: 0.1514 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1972 - mae: 0.3492 - val_loss: 0.0253 - val_mae: 0.1553 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2915 - mae: 0.4200 - val_loss: 0.0140 - val_mae: 0.1139 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2466 - mae: 0.3916 - val_loss: 0.0136 - val_mae: 0.1125 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1979 - mae: 0.3554 - val_loss: 0.0135 - val_mae: 0.1103 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2355 - mae: 0.3908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:07,726 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2329 - mae: 0.3864 - val_loss: 0.0019 - val_mae: 0.0369 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2509 - mae: 0.4003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:07,993 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2495 - mae: 0.4080 - val_loss: 0.0010 - val_mae: 0.0263 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2499 - mae: 0.3938 - val_loss: 0.0117 - val_mae: 0.1064 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1820 - mae: 0.3384 - val_loss: 0.0089 - val_mae: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2473 - mae: 0.3796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:08,704 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2188 - mae: 0.3550 - val_loss: 6.8157e-04 - val_mae: 0.0226 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2599 - mae: 0.3918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:08,966 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2460 - mae: 0.3824 - val_loss: 5.7923e-04 - val_mae: 0.0179 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2513 - mae: 0.3796 - val_loss: 0.0095 - val_mae: 0.0960 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2289 - mae: 0.3775 - val_loss: 0.0053 - val_mae: 0.0707 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2016 - mae: 0.3556 - val_loss: 0.0104 - val_mae: 0.1006 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2218 - mae: 0.3656 - val_loss: 0.0078 - val_mae: 0.0868 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2097 - mae: 0.3577 - val_loss: 6.9286e-04 - val_mae: 0.0204 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2264 - mae: 0.3774 - val_loss: 0.0026 - val_mae: 0.0482 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1893 - mae: 0.3390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:10,645 - absl - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2225 - mae: 0.3593 - val_loss: 3.2613e-04 - val_mae: 0.0126 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2706 - mae: 0.3972 - val_loss: 0.0034 - val_mae: 0.0552 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1847 - mae: 0.3197 - val_loss: 0.0026 - val_mae: 0.0474 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1820 - mae: 0.3203 - val_loss: 0.0037 - val_mae: 0.0582 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1882 - mae: 0.3343 - val_loss: 0.0069 - val_mae: 0.0809 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1412 - mae: 0.2939 - val_loss: 0.0054 - val_mae: 0.0714 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1470 - mae: 0.3055 - val_loss: 8.0127e-04 - val_mae: 0.0226 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1554 - mae: 0.3109 - val_loss: 0.0014 - val_mae: 0.0352 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1242 - mae: 0.2710 - val_loss: 6.2501e-04 - val_mae: 0.0193 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1245 - mae: 0.2713 - val_loss: 0.0018 - val_mae: 0.0379 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1399 - mae: 0.2852 - val_loss: 0.0011 - val_mae: 0.0287 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1500 - mae: 0.2941 - val_loss: 8.1755e-04 - val_mae: 0.0230 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1380 - mae: 0.2790 - val_loss: 4.6347e-04 - val_mae: 0.0160 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0996 - mae: 0.2546 - val_loss: 3.5497e-04 - val_mae: 0.0138 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1487 - mae: 0.2927 - val_loss: 0.0010 - val_mae: 0.0266 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1179 - mae: 0.2715 - val_loss: 0.0016 - val_mae: 0.0360 - learning_rate: 5.0000e-04\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:14,657 - models - INFO - LSTM - Train RMSE: 0.0191\n",
      "2025-11-14 23:10:14,657 - models - INFO - LSTM - Val RMSE: 0.0181\n",
      "2025-11-14 23:10:14,658 - main - INFO - LSTM training completed\n",
      "2025-11-14 23:10:14,658 - main - INFO - Training Ensemble model...\n",
      "2025-11-14 23:10:14,658 - models - INFO - Training ensemble models\n",
      "2025-11-14 23:10:14,658 - models - INFO - Training linear regression model\n",
      "2025-11-14 23:10:14,663 - models - INFO - Linear Regression - Train RMSE: 0.0157\n",
      "2025-11-14 23:10:14,663 - models - INFO - Linear Regression - Val RMSE: 0.0313\n",
      "2025-11-14 23:10:14,664 - models - INFO - Training Random Forest model\n",
      "2025-11-14 23:10:14,815 - models - INFO - Random Forest - Train RMSE: 0.0095\n",
      "2025-11-14 23:10:14,816 - models - INFO - Random Forest - Val RMSE: 0.0132\n",
      "2025-11-14 23:10:14,817 - models - INFO - Training Gradient Boosting model\n",
      "2025-11-14 23:10:15,086 - models - INFO - Gradient Boosting - Train RMSE: 0.0015\n",
      "2025-11-14 23:10:15,086 - models - INFO - Gradient Boosting - Val RMSE: 0.0146\n",
      "2025-11-14 23:10:15,101 - models - INFO - Ensemble - Val RMSE: 0.0145\n",
      "2025-11-14 23:10:15,102 - models - INFO - Ensemble weights: {'linear_regression': 0.18172692926890932, 'random_forest': 0.42963648824195366, 'gradient_boosting': 0.38863658248913696}\n",
      "2025-11-14 23:10:15,102 - main - INFO - Ensemble training completed\n",
      "2025-11-14 23:10:15,102 - main - INFO - Step 4: Evaluating models...\n",
      "2025-11-14 23:10:15,103 - main - INFO - Generated predictions for linear_regression\n",
      "2025-11-14 23:10:15,118 - main - INFO - Generated predictions for random_forest\n",
      "2025-11-14 23:10:15,118 - main - INFO - Generated predictions for gradient_boosting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:15,529 - main - INFO - Generated predictions for lstm\n",
      "2025-11-14 23:10:15,530 - main - ERROR - Error predicting with lstm_history: 'dict' object has no attribute 'predict'\n",
      "2025-11-14 23:10:15,545 - main - INFO - Generated predictions for ensemble\n",
      "2025-11-14 23:10:15,547 - main - INFO - \n",
      "Model Performance Summary:\n",
      "2025-11-14 23:10:15,548 - main - INFO - --------------------------------------------------\n",
      "2025-11-14 23:10:15,549 - main - INFO - \n",
      "Best performing model: random_forest\n",
      "2025-11-14 23:10:15,550 - main - INFO - Step 5: Creating visualizations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model      RMSE       MAE        R2  Directional_Accuracy\n",
      "1      random_forest  0.017628  0.012722 -0.099335              0.654321\n",
      "2  gradient_boosting  0.019461  0.014562 -0.339929              0.469136\n",
      "4           ensemble  0.022169  0.017782 -0.738668              0.518519\n",
      "3               lstm  0.034236  0.026254 -3.392621              0.434783\n",
      "0  linear_regression  0.052676  0.046665 -8.816488              0.469136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:10:15,846 - main - ERROR - Error creating visualizations: 'Train_RMSE'\n",
      "2025-11-14 23:10:15,847 - main - INFO - Step 6: Saving results...\n",
      "2025-11-14 23:10:15,847 - models - INFO - Model linear_regression saved to /Users/charlie/PredictiveStockAnalysis/models/AAPL_linear_regression_20251114\n",
      "2025-11-14 23:10:15,848 - main - INFO - Saved linear_regression model\n",
      "2025-11-14 23:10:15,856 - models - INFO - Model random_forest saved to /Users/charlie/PredictiveStockAnalysis/models/AAPL_random_forest_20251114\n",
      "2025-11-14 23:10:15,856 - main - INFO - Saved random_forest model\n",
      "2025-11-14 23:10:15,858 - models - INFO - Model gradient_boosting saved to /Users/charlie/PredictiveStockAnalysis/models/AAPL_gradient_boosting_20251114\n",
      "2025-11-14 23:10:15,859 - main - INFO - Saved gradient_boosting model\n",
      "2025-11-14 23:10:15,859 - main - ERROR - Error saving lstm: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/Users/charlie/PredictiveStockAnalysis/models/AAPL_lstm_20251114.\n",
      "2025-11-14 23:10:15,859 - models - INFO - Model lstm_history saved to /Users/charlie/PredictiveStockAnalysis/models/AAPL_lstm_history_20251114\n",
      "2025-11-14 23:10:15,860 - main - INFO - Saved lstm_history model\n",
      "2025-11-14 23:10:15,870 - models - INFO - Model ensemble saved to /Users/charlie/PredictiveStockAnalysis/models/AAPL_ensemble_20251114\n",
      "2025-11-14 23:10:15,870 - main - INFO - Saved ensemble model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StockPredictionPipeline\n\u001b[32m      2\u001b[39m pipeline = StockPredictionPipeline(symbol=\u001b[33m\"\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m\"\u001b[39m, start_date=\u001b[33m\"\u001b[39m\u001b[33m2021-01-01\u001b[39m\u001b[33m\"\u001b[39m, end_date=\u001b[33m\"\u001b[39m\u001b[33m2024-12-31\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/main.py:84\u001b[39m, in \u001b[36mStockPredictionPipeline.run_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m.create_visualizations()\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Step 6: Save Results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     87\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mPipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/main.py:338\u001b[39m, in \u001b[36mStockPredictionPipeline.save_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Save predictions\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictions:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     predictions_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     predictions_df[\u001b[33m'\u001b[39m\u001b[33mactual\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.y_test[:\u001b[38;5;28mlen\u001b[39m(predictions_df)]\n\u001b[32m    340\u001b[39m     predictions_path = os.path.join(config.OUTPUT_DIR,\n\u001b[32m    341\u001b[39m                                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_predictions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from main import StockPredictionPipeline\n",
    "pipeline = StockPredictionPipeline(symbol=\"AAPL\", start_date=\"2021-01-01\", end_date=\"2024-12-31\")\n",
    "results = pipeline.run_pipeline()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
