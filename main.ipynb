{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632e0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Pipeline for Stock Price Prediction Project\n",
    "Orchestrates data collection, feature engineering, model training, and evaluation\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from stocks import StockDataCollector\n",
    "from feature_engineering import FeatureEngineer\n",
    "from models import StockPredictionModels\n",
    "from visualization import StockVisualizer\n",
    "import config\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=config.LOGGING_CONFIG['level'],\n",
    "    format=config.LOGGING_CONFIG['format'],\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOGGING_CONFIG['log_file']),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class StockPredictionPipeline:\n",
    "    \"\"\"Main pipeline for stock prediction workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: str, start_date: str = None, end_date: str = None):\n",
    "        \"\"\"\n",
    "        Initialize pipeline\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date for data collection\n",
    "            end_date: End date for data collection\n",
    "        \"\"\"\n",
    "        self.symbol = symbol\n",
    "        self.start_date = start_date or config.DATA_CONFIG['start_date']\n",
    "        self.end_date = end_date or config.DATA_CONFIG['end_date']\n",
    "        \n",
    "        # Initialize components\n",
    "        self.collector = StockDataCollector(symbol, start_date, end_date)\n",
    "        self.engineer = FeatureEngineer(scaling_method=config.FEATURE_CONFIG['scaling_method'])\n",
    "        self.trainer = StockPredictionModels(config.MODEL_CONFIG)\n",
    "        self.visualizer = StockVisualizer()\n",
    "        \n",
    "        # Storage for results\n",
    "        self.raw_data = None\n",
    "        self.engineered_data = None\n",
    "        self.predictions = {}\n",
    "        self.performance = {}\n",
    "        \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        logger.info(f\"Starting pipeline for {self.symbol}\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        # Step 1: Data Collection\n",
    "        self.collect_data()\n",
    "        \n",
    "        # Step 2: Feature Engineering\n",
    "        self.engineer_features()\n",
    "        \n",
    "        # Step 3: Model Training\n",
    "        self.train_models()\n",
    "        \n",
    "        # Step 4: Evaluation\n",
    "        self.evaluate_models()\n",
    "        \n",
    "        # Step 5: Visualization\n",
    "        self.create_visualizations()\n",
    "        \n",
    "        # Step 6: Save Results\n",
    "        self.save_results()\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        \n",
    "        return self.performance\n",
    "    \n",
    "    def collect_data(self):\n",
    "        \"\"\"Collect and prepare stock data\"\"\"\n",
    "        logger.info(\"Step 1: Collecting stock data...\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch raw data\n",
    "            self.raw_data = self.collector.fetch_stock_data()\n",
    "            logger.info(f\"Collected {len(self.raw_data)} days of data\")\n",
    "            \n",
    "            # Calculate technical indicators\n",
    "            self.collector.calculate_moving_averages()\n",
    "            self.collector.calculate_rsi()\n",
    "            self.collector.calculate_macd()\n",
    "            self.collector.calculate_bollinger_bands()\n",
    "            self.collector.calculate_volatility()\n",
    "            self.collector.calculate_volume_indicators()\n",
    "            self.collector.add_price_features()\n",
    "            \n",
    "            self.raw_data = self.collector.data\n",
    "            \n",
    "            # Save raw data\n",
    "            data_path = os.path.join(config.DATA_DIR, \n",
    "                                     f\"{self.symbol}_raw_data_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "            self.raw_data.to_csv(data_path, index=False)\n",
    "            logger.info(f\"Raw data saved to {data_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data collection: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def engineer_features(self):\n",
    "        \"\"\"Engineer features for model training\"\"\"\n",
    "        logger.info(\"Step 2: Engineering features...\")\n",
    "        \n",
    "        try:\n",
    "            # Create advanced features\n",
    "            self.engineered_data = self.engineer.engineer_all_features(\n",
    "                self.raw_data,\n",
    "                target_col='Close',\n",
    "                sequence_length=config.DATA_CONFIG['sequence_length']\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created {len(self.engineered_data['feature_names'])} features\")\n",
    "            logger.info(f\"Selected top {len(self.engineered_data['selected_features'])} features\")\n",
    "            \n",
    "            # Save feature names\n",
    "            features_path = os.path.join(config.DATA_DIR, \n",
    "                                        f\"{self.symbol}_features_{datetime.now().strftime('%Y%m%d')}.json\")\n",
    "            with open(features_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'all_features': self.engineered_data['feature_names'],\n",
    "                    'selected_features': self.engineered_data['selected_features']\n",
    "                }, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in feature engineering: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        logger.info(\"Step 3: Training models...\")\n",
    "        \n",
    "        # Prepare data splits\n",
    "        features = self.engineered_data['features']\n",
    "        target = self.engineered_data['target']\n",
    "        features_lstm = self.engineered_data['features_lstm']\n",
    "        target_lstm = self.engineered_data['target_lstm']\n",
    "        \n",
    "        # Calculate split indices\n",
    "        train_size = int(len(features) * (1 - config.DATA_CONFIG['test_size']))\n",
    "        val_size = int(train_size * config.DATA_CONFIG['validation_size'])\n",
    "        \n",
    "        # Traditional ML data split\n",
    "        X_train = features[:train_size-val_size].values\n",
    "        y_train = target[:train_size-val_size].values\n",
    "        X_val = features[train_size-val_size:train_size].values\n",
    "        y_val = target[train_size-val_size:train_size].values\n",
    "        X_test = features[train_size:].values\n",
    "        y_test = target[train_size:].values\n",
    "        \n",
    "        # LSTM data split\n",
    "        lstm_train_size = int(len(features_lstm) * (1 - config.DATA_CONFIG['test_size']))\n",
    "        lstm_val_size = int(lstm_train_size * config.DATA_CONFIG['validation_size'])\n",
    "        \n",
    "        X_train_lstm = features_lstm[:lstm_train_size-lstm_val_size]\n",
    "        y_train_lstm = target_lstm[:lstm_train_size-lstm_val_size]\n",
    "        X_val_lstm = features_lstm[lstm_train_size-lstm_val_size:lstm_train_size]\n",
    "        y_val_lstm = target_lstm[lstm_train_size-lstm_val_size:lstm_train_size]\n",
    "        X_test_lstm = features_lstm[lstm_train_size:]\n",
    "        y_test_lstm = target_lstm[lstm_train_size:]\n",
    "        \n",
    "        # Store test data for later evaluation\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_lstm = X_test_lstm\n",
    "        self.y_test_lstm = y_test_lstm\n",
    "        \n",
    "        # Train models\n",
    "        models_to_train = [\n",
    "            ('Linear Regression', 'linear'),\n",
    "            ('Random Forest', 'random_forest'),\n",
    "            ('Gradient Boosting', 'gradient_boosting')\n",
    "        ]\n",
    "        \n",
    "        for model_name, model_type in models_to_train:\n",
    "            try:\n",
    "                logger.info(f\"Training {model_name}...\")\n",
    "                \n",
    "                if model_type == 'linear':\n",
    "                    self.trainer.train_linear_regression(X_train, y_train, X_val, y_val)\n",
    "                elif model_type == 'random_forest':\n",
    "                    self.trainer.train_random_forest(X_train, y_train, X_val, y_val)\n",
    "                elif model_type == 'gradient_boosting':\n",
    "                    self.trainer.train_gradient_boosting(X_train, y_train, X_val, y_val)\n",
    "                \n",
    "                logger.info(f\"{model_name} training completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Train LSTM if enabled\n",
    "        if config.TRAINING_CONFIG.get('train_lstm', True):\n",
    "            try:\n",
    "                logger.info(\"Training LSTM model...\")\n",
    "                self.trainer.train_lstm(X_train_lstm, y_train_lstm, \n",
    "                                       X_val_lstm, y_val_lstm)\n",
    "                logger.info(\"LSTM training completed\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training LSTM: {str(e)}\")\n",
    "        \n",
    "        # Train ensemble\n",
    "        try:\n",
    "            logger.info(\"Training Ensemble model...\")\n",
    "            self.trainer.train_ensemble(X_train, y_train, X_val, y_val)\n",
    "            logger.info(\"Ensemble training completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training Ensemble: {str(e)}\")\n",
    "    \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate all trained models\"\"\"\n",
    "        logger.info(\"Step 4: Evaluating models...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        for model_name in self.trainer.models.keys():\n",
    "            try:\n",
    "                if 'lstm' in model_name.lower() or 'gru' in model_name.lower():\n",
    "                    self.predictions[model_name] = self.trainer.predict(\n",
    "                        model_name, self.X_test_lstm\n",
    "                    )\n",
    "                else:\n",
    "                    self.predictions[model_name] = self.trainer.predict(\n",
    "                        model_name, self.X_test\n",
    "                    )\n",
    "                \n",
    "                logger.info(f\"Generated predictions for {model_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error predicting with {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Calculate performance metrics for each model\n",
    "        comparison_data = []\n",
    "        for model_name, predictions in self.predictions.items():\n",
    "            if 'lstm' in model_name.lower():\n",
    "                metrics = self.trainer.calculate_metrics(self.y_test_lstm, predictions)\n",
    "            else:\n",
    "                metrics = self.trainer.calculate_metrics(self.y_test, predictions)\n",
    "\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'RMSE': metrics['rmse'],\n",
    "                'MAE': metrics['mae'],\n",
    "                'R2': metrics['r2'],\n",
    "                'Directional_Accuracy': metrics['directional_accuracy']\n",
    "            })\n",
    "\n",
    "        self.performance = pd.DataFrame(comparison_data).sort_values('RMSE')\n",
    "\n",
    "        \n",
    "        logger.info(\"\\nModel Performance Summary:\")\n",
    "        logger.info(\"-\" * 50)\n",
    "        print(self.performance.to_string())\n",
    "        \n",
    "        # Identify best model\n",
    "        best_model = self.performance.iloc[0]['Model']\n",
    "        logger.info(f\"\\nBest performing model: {best_model}\")\n",
    "        \n",
    "        # Save model comparison\n",
    "        comparison_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                      f\"{self.symbol}_model_comparison_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "        self.performance.to_csv(comparison_path, index=False)\n",
    "        \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"Create and save visualizations\"\"\"\n",
    "        logger.info(\"Step 5: Creating visualizations...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for visualization\n",
    "            viz_data = {\n",
    "                'stock_data': self.raw_data,\n",
    "                'symbol': self.symbol,\n",
    "                'indicators': ['SMA_20', 'SMA_50', 'SMA_200'],\n",
    "                'predictions': self.predictions,\n",
    "                'actual': self.y_test if len(self.predictions) > 0 else None,\n",
    "                'comparison': self.performance,\n",
    "                'training_history': self.trainer.models.get('lstm_history', {})\n",
    "            }\n",
    "            \n",
    "            # Create visualizations\n",
    "            # Stock price plot\n",
    "            stock_fig = self.visualizer.plot_stock_price(\n",
    "                self.raw_data,\n",
    "                self.symbol,\n",
    "                ['SMA_20', 'SMA_50', 'SMA_200']\n",
    "            )\n",
    "\n",
    "            # Predictions plot\n",
    "            if self.predictions:\n",
    "                predictions_fig = self.visualizer.plot_predictions(\n",
    "                    self.y_test,\n",
    "                    self.predictions\n",
    "                )\n",
    "\n",
    "            # Model comparison\n",
    "            if not self.performance.empty:\n",
    "                comparison_fig = self.visualizer.plot_model_comparison(self.performance)\n",
    "            \n",
    "            logger.info(f\"Visualizations saved to {config.OUTPUT_DIR}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save all results and models\"\"\"\n",
    "        logger.info(\"Step 6: Saving results...\")\n",
    "        \n",
    "        # Save models\n",
    "        for model_name in self.trainer.models.keys():\n",
    "            try:\n",
    "                model_path = os.path.join(config.MODEL_DIR,\n",
    "                                         f\"{self.symbol}_{model_name}_{datetime.now().strftime('%Y%m%d')}\")\n",
    "                self.trainer.save_model(model_name, model_path)\n",
    "                logger.info(f\"Saved {model_name} model\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Save predictions\n",
    "        if self.predictions:\n",
    "            predictions_df = pd.DataFrame(self.predictions)\n",
    "            predictions_df['actual'] = self.y_test[:len(predictions_df)]\n",
    "            predictions_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                           f\"{self.symbol}_predictions_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "            predictions_df.to_csv(predictions_path, index=False)\n",
    "            logger.info(f\"Predictions saved to {predictions_path}\")\n",
    "        \n",
    "        # Save summary report\n",
    "        self.generate_report()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate summary report\"\"\"\n",
    "        report_path = os.path.join(config.OUTPUT_DIR,\n",
    "                                  f\"{self.symbol}_report_{datetime.now().strftime('%Y%m%d')}.txt\")\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"Stock Price Prediction Report\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            f.write(f\"Symbol: {self.symbol}\\n\")\n",
    "            f.write(f\"Date Range: {self.start_date} to {self.end_date}\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Data Statistics:\\n\")\n",
    "            f.write(f\"- Total days: {len(self.raw_data)}\\n\")\n",
    "            f.write(f\"- Features created: {len(self.engineered_data['feature_names'])}\\n\")\n",
    "            f.write(f\"- Features selected: {len(self.engineered_data['selected_features'])}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Model Performance:\\n\")\n",
    "            f.write(self.performance.to_string())\n",
    "            f.write(f\"\\n\\nBest Model: {self.performance.iloc[0]['Model']}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nTop 10 Selected Features:\\n\")\n",
    "            for i, feature in enumerate(self.engineered_data['selected_features'][:10]):\n",
    "                f.write(f\"  {i+1}. {feature}\\n\")\n",
    "\n",
    "        logger.info(f\"Report saved to {report_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the pipeline\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Stock Price Prediction Pipeline')\n",
    "\n",
    "    parser.add_argument('symbol', type=str, help='Stock symbol (e.g., AAPL, GOOGL)')\n",
    "    parser.add_argument('--start-date', type=str, help='Start date (YYYY-MM-DD)')\n",
    "    parser.add_argument('--end-date', type=str, help='End date (YYYY-MM-DD)')\n",
    "    parser.add_argument('--dashboard', action='store_true', help='Launch dashboard after training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = StockPredictionPipeline(\n",
    "        symbol=args.symbol,\n",
    "        start_date=args.start_date,\n",
    "        end_date=args.end_date\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    try:\n",
    "        results = pipeline.run_pipeline()\n",
    "\n",
    "        # Launch dashboard if requested\n",
    "        if args.dashboard:\n",
    "            logger.info(\"Launching dashboard...\")\n",
    "            from dashboard import app\n",
    "            app.run(\n",
    "                debug=config.DASHBOARD_CONFIG['debug'],\n",
    "                host=config.DASHBOARD_CONFIG['host'],\n",
    "                port=config.DASHBOARD_CONFIG['port']\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e3f2ce-1edd-47e0-a39b-da32b0723fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 22:47:34,726 - main - INFO - Starting pipeline for AAPL\n",
      "2025-11-14 22:47:34,726 - main - INFO - ==================================================\n",
      "2025-11-14 22:47:34,726 - main - INFO - Step 1: Collecting stock data...\n",
      "2025-11-14 22:47:34,727 - stocks - INFO - Fetching data for AAPL from 2021-01-01 to 2024-12-31 (interval=1d)\n",
      "2025-11-14 22:47:35,441 - stocks - INFO - Successfully fetched 1004 rows\n",
      "2025-11-14 22:47:35,442 - main - INFO - Collected 1004 days of data\n",
      "2025-11-14 22:47:35,445 - stocks - INFO - Calculated moving averages for windows: [10, 20, 50, 200]\n",
      "2025-11-14 22:47:35,446 - stocks - INFO - Calculated RSI_14\n",
      "2025-11-14 22:47:35,448 - stocks - INFO - Calculated MACD (12, 26, 9)\n",
      "2025-11-14 22:47:35,449 - stocks - INFO - Calculated Bollinger Bands (window=20, std=2.0)\n",
      "2025-11-14 22:47:35,451 - stocks - INFO - Calculated volatility metrics (window=20)\n",
      "2025-11-14 22:47:35,465 - stocks - INFO - Calculated volume indicators\n",
      "2025-11-14 22:47:35,466 - stocks - INFO - Added price/lag features with lags=[1, 3, 5, 10]\n",
      "2025-11-14 22:47:35,492 - main - INFO - Raw data saved to /Users/charlie/PredictiveStockAnalysis/data/AAPL_raw_data_20251114.csv\n",
      "2025-11-14 22:47:35,492 - main - INFO - Step 2: Engineering features...\n",
      "2025-11-14 22:47:35,492 - feature_engineering - INFO - Starting complete feature engineering pipeline\n",
      "2025-11-14 22:47:35,493 - feature_engineering - INFO - Creating advanced technical features...\n",
      "2025-11-14 22:47:35,857 - feature_engineering - INFO - Created 111 total features\n",
      "2025-11-14 22:47:35,858 - feature_engineering - INFO - Creating lagged features for 3 columns\n",
      "2025-11-14 22:47:35,860 - feature_engineering - INFO - Creating rolling features for 3 columns\n",
      "2025-11-14 22:47:35,866 - feature_engineering - INFO - Creating target features for horizons [1, 5, 10]\n",
      "2025-11-14 22:47:35,873 - feature_engineering - INFO - Removing features with correlation > 0.95\n",
      "2025-11-14 22:47:35,890 - feature_engineering - INFO - Removed 75 highly correlated features\n",
      "2025-11-14 22:47:35,891 - feature_engineering - INFO - Selecting top 50 features using mutual_info method\n",
      "2025-11-14 22:47:35,896 - main - ERROR - Error in feature engineering: name 'SelectKBest' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SelectKBest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StockPredictionPipeline\n\u001b[32m      2\u001b[39m pipeline = StockPredictionPipeline(symbol=\u001b[33m\"\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m\"\u001b[39m, start_date=\u001b[33m\"\u001b[39m\u001b[33m2021-01-01\u001b[39m\u001b[33m\"\u001b[39m, end_date=\u001b[33m\"\u001b[39m\u001b[33m2024-12-31\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/main.py:72\u001b[39m, in \u001b[36mStockPredictionPipeline.run_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.collect_data()\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Step 2: Feature Engineering\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Step 3: Model Training\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.train_models()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/main.py:127\u001b[39m, in \u001b[36mStockPredictionPipeline.engineer_features\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Engineering features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Create advanced features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28mself\u001b[39m.engineered_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengineer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengineer_all_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mClose\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDATA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msequence_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.engineered_data[\u001b[33m'\u001b[39m\u001b[33mfeature_names\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    134\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.engineered_data[\u001b[33m'\u001b[39m\u001b[33mselected_features\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/feature_engineering.py:767\u001b[39m, in \u001b[36mFeatureEngineer.engineer_all_features\u001b[39m\u001b[34m(self, df, target_col, sequence_length)\u001b[39m\n\u001b[32m    764\u001b[39m X = \u001b[38;5;28mself\u001b[39m.remove_multicollinearity(X, threshold=\u001b[32m0.95\u001b[39m)\n\u001b[32m    766\u001b[39m \u001b[38;5;66;03m# Select top features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m X_selected, selected_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmutual_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[32m    770\u001b[39m X_scaled = \u001b[38;5;28mself\u001b[39m.scale_features(X_selected, fit=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PredictiveStockAnalysis/feature_engineering.py:509\u001b[39m, in \u001b[36mFeatureEngineer.select_features\u001b[39m\u001b[34m(self, X, y, method, k)\u001b[39m\n\u001b[32m    506\u001b[39m X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mmutual_info\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     selector = \u001b[43mSelectKBest\u001b[49m(score_func=mutual_info_regression, k=\u001b[38;5;28mmin\u001b[39m(k, X_clean.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m    510\u001b[39m     X_selected = selector.fit_transform(X_clean, y)\n\u001b[32m    511\u001b[39m     selected_features = X_clean.columns[selector.get_support()].tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'SelectKBest' is not defined"
     ]
    }
   ],
   "source": [
    "from main import StockPredictionPipeline\n",
    "pipeline = StockPredictionPipeline(symbol=\"AAPL\", start_date=\"2021-01-01\", end_date=\"2024-12-31\")\n",
    "results = pipeline.run_pipeline()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
